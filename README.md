# ML Training Infrastructure & Experiments

Personal repository for building production-grade ML training infrastructure and running experiments, primarily focused on language models.

## What is this?

This started as a fork of [nanoGPT](https://github.com/karpathy/nanoGPT) but evolved into a full training infrastructure project as I built out:
- Docker-based deployment to cloud GPUs (RunPod)
- Automated checkpoint syncing to Google Cloud Storage
- Experiment tracking and observability
- Custom training pipelines for pretraining, fine-tuning, and RL

The goal is to build a repeatable system for training models at scale while learning ML fundamentals hands-on, and eventually exploring advanced modern reasearch experiments and training methods.

## Current Status

**Phase 1: Infrastructure** (in progress)
- ‚úÖ Dockerized training pipeline with CUDA support
- ‚úÖ Cloud GPU deployment on RunPod
- ‚úÖ GCS integration for checkpoint management
- üöß Experiment tracking with W&B
- üöß Comprehensive eval infrastructure

**Next up:**
- Training observability & metrics
- GPT-2 reproduction on OpenWebText
- Fine-tuning (SFT, LoRA)
- RLHF implementations (GRPO, DPO, PPO)
- Open ended exploration

## Structure

TODO - make it look like ‚¨áÔ∏è

<!-- ```
‚îú‚îÄ‚îÄ infra/              # Docker, deployment scripts
‚îú‚îÄ‚îÄ nanogpt/            # Core training code (based on Karpathy's nanoGPT)
‚îú‚îÄ‚îÄ config/             # Training configurations
‚îú‚îÄ‚îÄ data/               # Dataset preparation scripts
‚îî‚îÄ‚îÄ experiments/        # Results and checkpoints (synced to GCS)
``` -->

## Why Public?

Building in public to document my learning and create a portfolio artifact. This represents real infrastructure work beyond toy notebooks - deploying to cloud GPUs, managing experiments, building reproducible training pipelines.

Feel free to explore, but this is primarily a personal learning project.